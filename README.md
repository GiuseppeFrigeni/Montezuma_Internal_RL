# Montezuma Internal RL

Course project for Reinforcement Learning focused on **hierarchical control in Montezuma's Revenge**.
This repository is my attempt to **replicate and adapt** the ideas from the paper
*Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning*
in the Montezuma Revenge setting.

The project combines:
- an **autoregressive vision-action sequence backbone** pretrained with next-action prediction on expert trajectories (behavior-cloning style),
- a **metacontroller** that injects latent low-rank adapters into the backbone,
- an **internal RL stage** that learns option-like latent decisions over time.

## Project Idea

Montezuma's Revenge is a sparse-reward environment where long-horizon exploration is hard for flat policies.

This repository explores a two-level setup:
1. learn a strong primitive policy model from offline trajectories,
2. learn a latent controller that decides when/how to adapt that policy through time.

The goal is to improve temporal abstraction (option-like behavior) while staying grounded in pixel observations.
In particular, this project follows the original paper's core hypothesis and tests it on Montezuma Revenge using
my training pipeline and implementation choices.

## Repository Structure

```text
Montezuma_Internal_RL/
├── src/
│   ├── data/
│   │   └── dataset.py                # Offline Atari dataset loader and sequence sampling
│   ├── models/
│   │   ├── vlm.py                    # Interleaved visual-action transformer backbone
│   │   └── metacontroller.py         # Hawk/LRU-based latent metacontroller + hypernetwork
│   ├── training/
│   │   ├── train_bc.py               # Stage 1: behavior cloning of the backbone
│   │   ├── train_meta.py             # Stage 2: metacontroller training
│   │   └── internal-rl.ipynb         # Stage 3: internal RL (option-level policy)
│   └── utils/
│       └── logger.py                 # CSV logger used by training scripts
├── play_internal_rl.py               # Evaluation / gameplay with trained checkpoints
├── requirements.txt
└── README.md
```

## Method Overview

### 1) Backbone Pretraining (`train_bc.py`)

`MontezumaVLM` processes a sequence of frame stacks and action history:
- inputs are 9 image snapshots (each snapshot stacks 4 grayscale frames),
- action tokens are interleaved in chunks,
- transformer predicts action logits at every action position,
- optional observation-prediction loss regularizes representation learning.

Training objective:
- action cross-entropy over all timesteps,
- plus optional weighted observation reconstruction in embedding space.

### 2) Metacontroller Training (`train_meta.py`)

The metacontroller reads backbone residual activations and produces latent variables `z_t`.
A hypernetwork decodes `z_t` into low-rank adapter matrices `(A_t, B_t)`, injected into a chosen backbone layer.

Key mechanisms:
- recurrent history state,
- sequence embedding via bidirectional Hawk blocks,
- latent encoder with KL regularization,
- switch unit (`beta`) to control latent refresh frequency.

Training objective:
- action NLL under adapted backbone,
- KL penalty (with optional annealing),
- optional auxiliary position loss.

### 3) Internal RL (`internal-rl.ipynb`)

An option-level policy operates in latent space and is trained with PPO-style updates:
- policy outputs latent controls,
- metacontroller switch dynamics determine option boundaries,
- primitive actions remain generated by the adapted backbone.

This stage optimizes long-horizon decision structure while reusing pretrained components.

## Data Format

The dataset loader expects:

```text
data/atari_v1/
├── trajectories/revenge/*.txt
└── screens/revenge/<traj_id>/<frame_id>.png
```

Notes:
- training samples are built from death-free trajectory segments,
- trajectory filtering by top score is supported (`--limit_trajs`),
- NOOP subsampling is enabled by default (can be disabled via CLI).
- dataset download (Montezuma Revenge split): [https://omnomnom.vision.rwth-aachen.de/data/atari_v1_release/revenge.tar.gz](https://omnomnom.vision.rwth-aachen.de/data/atari_v1_release/revenge.tar.gz)

## Setup

### 1) Create environment

```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### 2) Install Atari ROM support (Gymnasium / ALE)

`gymnasium[atari,accept-rom-license]` is already in `requirements.txt`.
If ROM issues appear, verify your ALE setup in the active environment.

## Training Pipeline

Run from repository root.

### Stage 1: Behavior Cloning

```bash
python src/training/train_bc.py \
  --epochs 10 \
  --batch_size 16 \
  --seq_len 64 \
  --lr 3e-4
```

Useful flags:
- `--obs_weight` weight for observation-prediction loss
- `--limit_trajs` keep top % trajectories by score
- `--no_noop_subsample` disable NOOP subsampling
- `--resume` resume from checkpoint

### Stage 2: Metacontroller

```bash
python src/training/train_meta.py \
  --checkpoint checkpoints/vlm_epoch_10.pt \
  --epochs 10 \
  --batch_size 16 \
  --seq_len 64 \
  --kl_weight 0.1 \
  --kl_anneal_steps 5000
```

Useful flags:
- `--switch_rate` force fixed latent switching every N steps
- `--position_weight` enable auxiliary position supervision
- `--hinged_position_loss` timestep-aligned position objective
- `--limit_trajs`, `--no_noop_subsample`, `--resume`

### Stage 3: Internal RL

Use `src/training/internal-rl.ipynb` for the PPO-style option-level training phase.

## Evaluation / Gameplay

```bash
python play_internal_rl.py \
  --vlm-checkpoint checkpoints/vlm_epoch_10.pt \
  --metacontroller-checkpoint checkpoints/meta_simple_epoch_10.pt \
  --episodes 3 \
  --render
```

Optional agent-mode evaluation (internal-RL policy checkpoint):

```bash
python play_internal_rl.py \
  --vlm-checkpoint <path_to_vlm.pt> \
  --metacontroller-checkpoint <path_to_meta.pt> \
  --agent-checkpoint <path_to_policy.pt> \
  --episodes 3
```

Useful flags:
- `--deterministic` for greedy action/latent selection
- `--max-steps` cap primitive steps per episode
- `--beta-threshold` switching threshold in agent mode
- `--switch-rate` fixed switching interval

## Reference Paper

- Kobayashi, S., Schimpf, Y., Schlegel, M., Steger, A., Wolczyk, M., von Oswald, J., Scherrer, N., Maile, K., Lajoie, G., Richards, B. A., Saurous, R. A., Manyika, J., Agüera y Arcas, B., Meulemans, A., & Sacramento, J. (2025). *Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning*. arXiv:2512.20605 [cs.LG]. [https://arxiv.org/abs/2512.20605](https://arxiv.org/abs/2512.20605)
- Kurin, V., Nowozin, S., Hofmann, K., Beyer, L., & Leibe, B. (2017). *The Atari Grand Challenge Dataset*. arXiv:1705.10998 [cs.AI]. [https://arxiv.org/abs/1705.10998](https://arxiv.org/abs/1705.10998)

## Reproducibility Notes

- Device auto-selection supports `mps`, `cuda`, then `cpu`.
- Random seeds can be set for evaluation (`play_internal_rl.py --seed ...`).
- Data quality and trajectory filtering substantially affect results in sparse-reward settings.

## License

No license file is currently provided.
