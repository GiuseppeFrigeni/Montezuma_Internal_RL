{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install required libraries for Atari environment\n",
    "!pip install -q gymnasium[atari,accept-rom-license] shimmy ale-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import argparse\n",
    "import csv\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import types\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "from torch.distributions import Normal\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from dataclasses import dataclass\n",
    "import ale_py\n",
    "\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "#Module mapping\n",
    "sys.modules['src'] = types.ModuleType('src')\n",
    "sys.modules['src.models'] = types.ModuleType('src.models')\n",
    "sys.modules['src.models.vlm'] = types.ModuleType('src.models.vlm')\n",
    "sys.modules['src.models.metacontroller'] = types.ModuleType('src.models.metacontroller')\n",
    "\n",
    "\n",
    "class TrainConfig:\n",
    "    # Paths\n",
    "    model_checkpoint = \"/kaggle/input/best-models-montezuma/vlm_best.pt\"\n",
    "    meta_checkpoint = \"/kaggle/input/best-models-montezuma/meta_simple_epoch_18.pt\"\n",
    "    save_path = \"policy_best_.pt\"\n",
    "    log_path = \"internal_rl_logs.csv\"\n",
    "    episode_log_path = \"episode_logs.csv\"   \n",
    "    \n",
    "    # Training Hyperparameters\n",
    "    epochs = 1000\n",
    "    batch_size = 64         # Trajectories per epoch\n",
    "    num_envs = 8            \n",
    "    lr = 3e-4\n",
    "    \n",
    "    # Model Params\n",
    "    hidden_dim = 256\n",
    "    input_dim = 256\n",
    "    latent_dim = 8\n",
    "    initial_std = 0.7\n",
    "    \n",
    "    # Hierarchy Settings\n",
    "    fixed_rate = 0\n",
    "    max_steps_without_reward = 500     #internal steps before timeout\n",
    "    \n",
    "    # VLM/Env Settings\n",
    "    seq_len = 64\n",
    "    img_size = 84\n",
    "    patch_size = 14\n",
    "    embed_dim = 256\n",
    "    n_layers = 6\n",
    "    n_heads = 8\n",
    "    frame_stack = 4\n",
    "    \n",
    "    # Optimization\n",
    "    entropy_coef = 0\n",
    "    \n",
    "    # Misc\n",
    "    render = False\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "cfg = TrainConfig()\n",
    "\n",
    "# ==========================================\n",
    "# 2. MODELS (VLM & Metacontroller)\n",
    "# ==========================================\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    img_size: int = 84\n",
    "    patch_size: int = 14\n",
    "    frame_stack: int = 4\n",
    "    embed_dim: int = 256\n",
    "    n_heads: int = 8\n",
    "    n_layers: int = 6\n",
    "    dropout: float = 0.1\n",
    "    seq_len: int = 64\n",
    "    n_actions: int = 18\n",
    "    duration_vocab_size: int = 65\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, in_chans, embed_dim):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, n_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = embed_dim // n_heads\n",
    "        self.qkv = nn.Linear(embed_dim, 3 * embed_dim)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, T, 3, self.n_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        out = F.scaled_dot_product_attention(\n",
    "            q, k, v, dropout_p=self.dropout if self.training else 0.0, is_causal=True\n",
    "        )\n",
    "        out = out.transpose(1, 2).reshape(B, T, C)\n",
    "        return self.proj(out)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, n_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = CausalSelfAttention(embed_dim, n_heads, dropout)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * embed_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class MontezumaVLM(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_dim = config.embed_dim\n",
    "        self.n_patches = (config.img_size // config.patch_size) ** 2\n",
    "        \n",
    "        self.patch_embed = PatchEmbedding(config.img_size, config.patch_size, config.frame_stack, config.embed_dim)\n",
    "        self.action_embed = nn.Embedding(config.n_actions + 1, config.embed_dim)\n",
    "        self.start_token_id = config.n_actions\n",
    "        \n",
    "        max_tokens = 512 \n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, max_tokens, config.embed_dim))\n",
    "        self.type_embed = nn.Embedding(2, config.embed_dim)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(config.embed_dim, config.n_heads, config.dropout)\n",
    "            for _ in range(config.n_layers)\n",
    "        ])\n",
    "        \n",
    "        self.ln_f = nn.LayerNorm(config.embed_dim)\n",
    "        self.action_head = nn.Linear(config.embed_dim, config.n_actions)\n",
    "        self.obs_head = nn.Sequential(\n",
    "            nn.Linear(config.embed_dim, config.embed_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config.embed_dim * 2, self.n_patches * config.embed_dim)\n",
    "        )\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.trunc_normal_(module.weight, std=0.02)\n",
    "                if module.bias is not None: nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                nn.init.trunc_normal_(module.weight, std=0.02)\n",
    "    \n",
    "    def forward(self, frames, actions, return_residuals=False, adapter_params=None):\n",
    "        \"\"\"\n",
    "        adapter_params (optional):\n",
    "          - None\n",
    "          - (A, B_mat, layer_idx, pos_spec)\n",
    "    \n",
    "        where:\n",
    "          A, B_mat: (B, D, R) low-rank factors from hypernet\n",
    "          layer_idx: int (which transformer block to intervene at)\n",
    "          pos_spec: either\n",
    "            - \"last_action\"  -> apply at the last action token position\n",
    "            - int            -> apply at that absolute token index\n",
    "            - None           -> (legacy) apply to all tokens (NOT recommended for paper-faithful)\n",
    "        \"\"\"\n",
    "        B, NumImg, C, H, W = frames.shape\n",
    "        device = frames.device\n",
    "    \n",
    "        # ---- vision tokens ----\n",
    "        frames_flat = frames.view(B * NumImg, C, H, W)\n",
    "        vis_tokens = self.patch_embed(frames_flat).view(B, NumImg, self.n_patches, self.embed_dim)\n",
    "    \n",
    "        # ---- action tokens ----\n",
    "        action_input = torch.cat(\n",
    "            [torch.full((B, 1), self.start_token_id, device=device, dtype=torch.long), actions[:, :-1]],\n",
    "            dim=1,\n",
    "        )\n",
    "        act_embeds = self.action_embed(action_input)\n",
    "    \n",
    "        # ---- interleave tokens and build positions ----\n",
    "        token_list, type_list = [], []\n",
    "        action_positions = []\n",
    "        num_actions = act_embeds.shape[1]\n",
    "        current_pos = 0\n",
    "    \n",
    "        for i in range(NumImg):\n",
    "            token_list.append(vis_tokens[:, i])  # (B, n_patches, D)\n",
    "            type_list.append(torch.zeros(self.n_patches, dtype=torch.long, device=device))\n",
    "            current_pos += self.n_patches\n",
    "    \n",
    "            start_idx = i * 8\n",
    "            end_idx = min((i + 1) * 8, num_actions)\n",
    "            if start_idx < num_actions:\n",
    "                chunk = act_embeds[:, start_idx:end_idx]  # (B, k, D)\n",
    "                token_list.append(chunk)\n",
    "                type_list.append(torch.ones(chunk.shape[1], dtype=torch.long, device=device))\n",
    "    \n",
    "                for j in range(chunk.shape[1]):\n",
    "                    action_positions.append(current_pos + j)\n",
    "                current_pos += chunk.shape[1]\n",
    "    \n",
    "        tokens = torch.cat(token_list, dim=1)  # (B, T, D)\n",
    "        seq_len_curr = tokens.shape[1]\n",
    "    \n",
    "        tokens = tokens + self.pos_embed[:, :seq_len_curr]\n",
    "        tokens = tokens + self.type_embed(torch.cat(type_list))\n",
    "    \n",
    "        # Determine last action position *before* blocks so adapters can use it.\n",
    "        if len(action_positions) == 0:\n",
    "            # Should not happen if seq_len >= 1, but guard anyway.\n",
    "            last_action_pos = seq_len_curr - 1\n",
    "        else:\n",
    "            last_action_pos = int(action_positions[-1])\n",
    "    \n",
    "        residuals = []\n",
    "        x = tokens\n",
    "    \n",
    "        # ---- transformer blocks (optionally intervene at one layer + one position) ----\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            x = block(x)\n",
    "    \n",
    "            if adapter_params is not None:\n",
    "                A, B_mat, layer_idx, pos_spec = adapter_params\n",
    "                if i == layer_idx:\n",
    "                    # Choose position to intervene\n",
    "                    if pos_spec is None:\n",
    "                        if A.dim() == 3:\n",
    "                            term1 = torch.einsum(\"btd,bdr->btr\", x, B_mat)\n",
    "                            term2 = torch.einsum(\"btr,bdr->btd\", term1, A)\n",
    "                            x = x + term2\n",
    "                        elif A.dim() == 4:\n",
    "                            term1 = torch.einsum(\"btd,btdr->btr\", x, B_mat)\n",
    "                            term2 = torch.einsum(\"btr,btdr->btd\", term1, A)\n",
    "                            x = x + term2\n",
    "                        else:\n",
    "                            raise ValueError(f\"Bad adapter A shape: {A.shape}\")\n",
    "\n",
    "                    elif pos_spec == \"action_tokens\":\n",
    "                        # apply tokenwise A/B only on action token indices (not on patch tokens)\n",
    "                        idx = torch.tensor(action_positions, device=device, dtype=torch.long)  # (T_act,)\n",
    "                        x_tok = x[:, idx, :]                     # (B, T_act, D)\n",
    "                    \n",
    "                        # expects A and B_mat to be (B, T_tokens, D, R)\n",
    "                        A_tok = A[:, idx, :, :]                  # (B, T_act, D, R)\n",
    "                        B_tok = B_mat[:, idx, :, :]              # (B, T_act, D, R)\n",
    "                    \n",
    "                        term1 = torch.einsum(\"btd,btdr->btr\", x_tok, B_tok)   # (B, T_act, R)\n",
    "                        term2 = torch.einsum(\"btr,btdr->btd\", term1, A_tok)   # (B, T_act, D)\n",
    "                        x[:, idx, :] = x_tok + term2\n",
    "                    else:\n",
    "                        if pos_spec == \"last_action\":\n",
    "                            pos = last_action_pos\n",
    "                        elif isinstance(pos_spec, int):\n",
    "                            pos = pos_spec\n",
    "                        else:\n",
    "                            raise ValueError(f\"Unsupported pos_spec: {pos_spec}\")\n",
    "    \n",
    "                        # Intervene only at x[:, pos, :]\n",
    "                        x_pos = x[:, pos, :]                         # (B, D)\n",
    "                        term1 = torch.einsum(\"bd,bdr->br\", x_pos, B_mat)  # (B, R) = x_pos @ B\n",
    "                        term2 = torch.einsum(\"br,bdr->bd\", term1, A)      # (B, D) = term1 @ A^T\n",
    "                        x[:, pos, :] = x_pos + term2\n",
    "    \n",
    "            if return_residuals:\n",
    "                residuals.append(x.clone())\n",
    "    \n",
    "        x = self.ln_f(x)\n",
    "    \n",
    "        # Compute logits at action token positions; return only the last action logits for sampling\n",
    "        action_positions_t = torch.tensor(action_positions, device=device, dtype=torch.long)\n",
    "        action_embeddings = x[:, action_positions_t, :]      # (B, n_action_tokens, D)\n",
    "        action_logits = self.action_head(action_embeddings)  # (B, n_action_tokens, n_actions)\n",
    "    \n",
    "        output = {\n",
    "            \"logits\": action_logits[:, -1, :],  # last action-token logits\n",
    "            \"final_embedding\": x[:, -1, :],\n",
    "            \"last_action_pos\": last_action_pos,\n",
    "        }\n",
    "        if return_residuals:\n",
    "            output[\"residuals\"] = residuals\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "# --- Metacontroller Components ---\n",
    "\n",
    "class LRU(nn.Module):\n",
    "    def __init__(self, d_input, d_state, n_heads=8):\n",
    "        super().__init__()\n",
    "        self.d_input, self.d_state = d_input, d_state\n",
    "        self.W_in = nn.Linear(d_input, d_state, bias=False)\n",
    "        self.log_lambda = nn.Parameter(torch.ones(d_state) * 2.0)\n",
    "        self.W_out = nn.Linear(d_state, d_input, bias=False)\n",
    "        self.D = nn.Parameter(torch.ones(d_input) * 0.1)\n",
    "\n",
    "    def forward(self, u, reverse=False):\n",
    "        B, T, _ = u.shape\n",
    "        if reverse: u = torch.flip(u, dims=[1])\n",
    "        lam = torch.sigmoid(self.log_lambda)\n",
    "        u_proj = self.W_in(u)\n",
    "        x = torch.zeros(B, self.d_state, device=u.device)\n",
    "        ys = []\n",
    "        for t in range(T):\n",
    "            x = lam * x + (1 - lam) * u_proj[:, t]\n",
    "            ys.append(x)\n",
    "        y = self.W_out(torch.stack(ys, dim=1)) + u * self.D\n",
    "        return torch.flip(y, dims=[1]) if reverse else y\n",
    "\n",
    "    def step(self, u_t, state=None):\n",
    "        lam = torch.sigmoid(self.log_lambda)\n",
    "        u_proj = self.W_in(u_t)\n",
    "        if state is None: state = torch.zeros_like(u_proj)\n",
    "        new_state = lam * state + (1 - lam) * u_proj\n",
    "        y_t = self.W_out(new_state) + u_t * self.D\n",
    "        return y_t, new_state\n",
    "\n",
    "class HawkBlock(nn.Module):\n",
    "    def __init__(self, d_model, d_state=256, n_heads=8, mlp_ratio=2):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.lru = LRU(d_model, d_state, n_heads)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * mlp_ratio),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model * mlp_ratio, d_model),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, reverse=False):\n",
    "        x = x + self.lru(self.norm1(x), reverse=reverse)\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "    def step(self, x_t, state=None):\n",
    "        residual = x_t\n",
    "        lru_out, new_state = self.lru.step(self.norm1(x_t), state)\n",
    "        x_t = residual + lru_out\n",
    "        x_t = x_t + self.mlp(self.norm2(x_t))\n",
    "        return x_t, new_state\n",
    "\n",
    "class BidirectionalHawk(nn.Module):\n",
    "    \"\"\"Bidirectional Hawk SSM for acausal sequence embedding.\"\"\"\n",
    "    def __init__(self, d_input, d_output, d_state=256, n_heads=8, n_layers=1, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(d_input, d_output) if d_input != d_output else nn.Identity()\n",
    "        self.forward_layers = nn.ModuleList([\n",
    "            HawkBlock(d_output, d_state, n_heads, mlp_ratio=2) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.backward_layers = nn.ModuleList([\n",
    "            HawkBlock(d_output, d_state, n_heads, mlp_ratio=2) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.output_proj = nn.Linear(d_output * 2, d_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)\n",
    "        h_fwd = x\n",
    "        for layer in self.forward_layers:\n",
    "            h_fwd = layer(h_fwd, reverse=False)\n",
    "        h_bwd = x\n",
    "        for layer in self.backward_layers:\n",
    "            h_bwd = layer(h_bwd, reverse=True)\n",
    "        combined = torch.cat([h_fwd[:, -1, :], h_bwd[:, 0, :]], dim=-1)\n",
    "        return self.output_proj(combined)\n",
    "\n",
    "\n",
    "class Metacontroller(nn.Module):\n",
    "    def __init__(self, config, base_embed_dim, aux_position_predictor=False):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.base_embed_dim = base_embed_dim\n",
    "        self.latent_dim = 8\n",
    "        self.rank = 16\n",
    "        self.n_h = 32\n",
    "        self.n_s = 32  # THIS WAS MISSING\n",
    "        self.aux_position_predictor = aux_position_predictor\n",
    "        \n",
    "        self.history_gru = nn.GRUCell(base_embed_dim, self.n_h)\n",
    "        \n",
    "        # THIS WAS MISSING - required for checkpoint loading\n",
    "        self.seq_embedder = BidirectionalHawk(\n",
    "            d_input=base_embed_dim,\n",
    "            d_output=self.n_s,\n",
    "            d_state=64,\n",
    "            n_heads=4,\n",
    "            n_layers=1,\n",
    "            dropout=0.0\n",
    "        )\n",
    "        \n",
    "        # THIS WAS MISSING\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(base_embed_dim + self.n_h + self.n_s, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2 * self.latent_dim)\n",
    "        )\n",
    "        \n",
    "        self.switch_net = nn.Sequential(\n",
    "            nn.Linear(base_embed_dim + self.n_h + self.latent_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            self.switch_net[-1].bias.fill_(1.0)\n",
    "            \n",
    "        self.hypernet = nn.Sequential(\n",
    "            nn.Linear(self.latent_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2 * base_embed_dim * self.rank)\n",
    "        )\n",
    "        \n",
    "        # THIS WAS MISSING\n",
    "        if aux_position_predictor:\n",
    "            self.position_predictor = nn.Sequential(\n",
    "                nn.Linear(self.latent_dim, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(32, 2)\n",
    "            )\n",
    "\n",
    "    def step_with_z(self, e_t, z, h_switch_prev):\n",
    "        B = e_t.shape[0]\n",
    "        switch_input = torch.cat([e_t, h_switch_prev, z], dim=1)\n",
    "\n",
    "        logit = self.switch_net(switch_input)          # (B,1)\n",
    "        beta_t = torch.sigmoid(logit)                  # (B,1)\n",
    "        h_switch_t = self.history_gru(e_t, h_switch_prev)\n",
    "        params = self.hypernet(z).view(B, 2, self.base_embed_dim, self.rank)\n",
    "        return beta_t, h_switch_t, (params[:, 0], params[:, 1]), logit\n",
    "        \n",
    "# ==========================================\n",
    "# 3. ENVIRONMENT WRAPPER\n",
    "# ==========================================\n",
    "\n",
    "class InternalRLWrapper(gym.Env):\n",
    "    def __init__(self, device=\"cpu\", beta_threshold=0.5, seq_len=64, render_mode=None, shared_models=None):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.beta_threshold = beta_threshold\n",
    "        self.env = gym.make(\"ALE/MontezumaRevenge-v5\", render_mode=render_mode, frameskip=1)\n",
    "\n",
    "        self.base_model, self.metacontroller, self.config = shared_models\n",
    "        self.seq_len = seq_len\n",
    "        self.control_layer = self.config.n_layers // 2\n",
    "\n",
    "        self.num_images = 9\n",
    "        self.frame_stack = 4\n",
    "        self.frame_buffer = deque(maxlen=self.num_images * self.frame_stack)\n",
    "\n",
    "        self.n_patches = (self.config.img_size // self.config.patch_size) ** 2\n",
    "\n",
    "        self.action_history = deque(maxlen=self.seq_len - 1)\n",
    "        # switch-state must persist across primitive steps inside an option\n",
    "        self.h_switch = None\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        obs, info = self.env.reset(seed=seed, options=options)\n",
    "        self._reset_buffers(obs)\n",
    "        self.h_switch = torch.zeros(1, self.metacontroller.n_h, device=self.device)\n",
    "        return self._get_current_embedding(), info\n",
    "\n",
    "    def primitive_step(self, action: int):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(int(action))\n",
    "        self._update_frame_buffer(obs)\n",
    "        self.action_history.append(int(action))\n",
    "        return reward, terminated, truncated, info\n",
    "\n",
    "    def _get_sequence_length(self):\n",
    "        n_patches = (self.config.img_size // self.config.patch_size) ** 2\n",
    "        return self.num_images * n_patches + self.seq_len\n",
    "\n",
    "    def _reset_buffers(self, obs):\n",
    "        self.frame_buffer.clear()\n",
    "        self.action_history.clear()\n",
    "    \n",
    "        frame = self._process_frame(obs)\n",
    "        for _ in range(self.num_images * self.frame_stack):\n",
    "            self.frame_buffer.append(frame)\n",
    "    \n",
    "        for _ in range(self.seq_len - 1):\n",
    "            self.action_history.append(0)\n",
    "\n",
    "\n",
    "    def _update_frame_buffer(self, obs):\n",
    "        self.frame_buffer.append(self._process_frame(obs))\n",
    "\n",
    "    def _process_frame(self, obs):\n",
    "        img = cv2.resize(obs, (84, 84))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        return torch.tensor(img, dtype=torch.float32, device=self.device) / 255.0\n",
    "\n",
    "    def _get_model_inputs(self):\n",
    "        frames_list = list(self.frame_buffer)\n",
    "        actions_list = list(self.action_history) + [0]  # placeholder for \"next action\"\n",
    "        \n",
    "        images = []\n",
    "        for i in range(self.num_images):\n",
    "            start = i * self.frame_stack\n",
    "            images.append(torch.stack(frames_list[start : start + self.frame_stack], dim=0))\n",
    "        frames = torch.stack(images, dim=0).unsqueeze(0)  # (1, NumImg, 4, 84, 84)\n",
    "        actions = torch.tensor(actions_list, dtype=torch.long, device=self.device).unsqueeze(0)\n",
    "        return frames, actions\n",
    "\n",
    "    def _get_embedding_tensor(self):\n",
    "        frames, actions = self._get_model_inputs()\n",
    "        with torch.no_grad():\n",
    "            out = self.base_model(frames, actions, return_residuals=True)  # no adapter (clean)\n",
    "            lap = out[\"last_action_pos\"]\n",
    "            e = out[\"residuals\"][self.control_layer][:, lap, :]\n",
    "        return e\n",
    "\n",
    "\n",
    "    def _get_current_embedding(self):\n",
    "        return self._get_embedding_tensor().detach().cpu().numpy().flatten()\n",
    "\n",
    "# ==========================================\n",
    "# 4. AGENT & UTILS\n",
    "# ==========================================\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_dim=256, latent_dim=8, hidden_dim=256, initial_std=0.5):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_dim, hidden_dim) if input_dim != hidden_dim else nn.Identity()\n",
    "        self.ssm = HawkBlock(hidden_dim, d_state=256, n_heads=8, mlp_ratio=2)\n",
    "\n",
    "        self.actor_mean = nn.Sequential(\n",
    "            layer_init(nn.Linear(hidden_dim, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, latent_dim), std=0.01)\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.ones(latent_dim) * np.log(initial_std))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, D) or (B, T, D)\n",
    "        returns a Normal over z with matching batch/time shape:\n",
    "          - if x is (B, D): mu is (B, Z)\n",
    "          - if x is (B, T, D): mu is (B, T, Z)\n",
    "        \"\"\"\n",
    "        x = self.input_proj(x)\n",
    "\n",
    "        if x.dim() == 2:\n",
    "            # treat as length-1 sequence for HawkBlock.forward\n",
    "            x = x.unsqueeze(1)  # (B, 1, D)\n",
    "            x = self.ssm(x, reverse=False)  # (B, 1, D)\n",
    "            x = x.squeeze(1)  # (B, D)\n",
    "        elif x.dim() == 3:\n",
    "            x = self.ssm(x, reverse=False)  # (B, T, D)\n",
    "        else:\n",
    "            raise ValueError(f\"Actor.forward expected (B,D) or (B,T,D), got {tuple(x.shape)}\")\n",
    "\n",
    "        mu = self.actor_mean(x)\n",
    "        std = torch.exp(self.log_std)\n",
    "\n",
    "        # broadcast std to match mu shape\n",
    "        while std.dim() < mu.dim():\n",
    "            std = std.unsqueeze(0)\n",
    "        std = std.expand_as(mu)\n",
    "\n",
    "        return Normal(mu, std), None\n",
    "\n",
    "    def step(self, x_t, state=None):\n",
    "        # x_t: (B, D)\n",
    "        x_t = self.input_proj(x_t)\n",
    "        x_t, new_state = self.ssm.step(x_t, state)  # x_t: (B, D)\n",
    "        mu = self.actor_mean(x_t)                   # (B, Z)\n",
    "        std = torch.exp(self.log_std)               # (Z,)\n",
    "        std = std.unsqueeze(0).expand_as(mu)        # (B, Z)\n",
    "        return Normal(mu, std), new_state\n",
    "\n",
    "\n",
    "class CSVLogger:\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.writer = None\n",
    "        self.fieldnames = None\n",
    "        self.buffer = []  # Accumulate rows here\n",
    "        \n",
    "        # Check if file exists and has content\n",
    "        file_exists = os.path.exists(filename) and os.path.getsize(filename) > 0\n",
    "        \n",
    "        if file_exists:\n",
    "            with open(filename, 'r') as f:\n",
    "                reader = csv.DictReader(f)\n",
    "                self.fieldnames = reader.fieldnames\n",
    "            self.file = open(filename, 'a', newline='')\n",
    "            if self.fieldnames:\n",
    "                self.writer = csv.DictWriter(self.file, fieldnames=self.fieldnames)\n",
    "        else:\n",
    "            self.file = open(filename, 'w', newline='')\n",
    "    \n",
    "    def log(self, data, step, flush_every=5):\n",
    "        # Initialize writer on first call\n",
    "        if self.writer is None:\n",
    "            self.fieldnames = list(data.keys())\n",
    "            self.writer = csv.DictWriter(self.file, fieldnames=self.fieldnames)\n",
    "            self.writer.writeheader()\n",
    "        \n",
    "        # Accumulate in buffer\n",
    "        self.buffer.append(data)\n",
    "        \n",
    "        # Write all buffered rows every N steps\n",
    "        if step % flush_every == 0:\n",
    "            for row in self.buffer:\n",
    "                self.writer.writerow(row)\n",
    "            self.file.flush()\n",
    "            self.buffer = []  # Clear buffer\n",
    "    \n",
    "    def close(self):\n",
    "        # Write any remaining buffered data\n",
    "        if self.buffer and self.writer:\n",
    "            for row in self.buffer:\n",
    "                self.writer.writerow(row)\n",
    "            self.file.flush()\n",
    "        if self.file:\n",
    "            self.file.close()\n",
    "\n",
    "def load_shared_models(device):\n",
    "    # Module mapping for unpickling\n",
    "    sys.modules['src.models.vlm'].Config = Config\n",
    "    sys.modules['src.models.vlm'].MontezumaVLM = MontezumaVLM\n",
    "    sys.modules['src.models.vlm'].PatchEmbedding = PatchEmbedding\n",
    "    sys.modules['src.models.vlm'].TransformerBlock = TransformerBlock\n",
    "    sys.modules['src.models.vlm'].CausalSelfAttention = CausalSelfAttention\n",
    "    \n",
    "    sys.modules['src.models.metacontroller'].Metacontroller = Metacontroller\n",
    "    sys.modules['src.models.metacontroller'].HawkBlock = HawkBlock\n",
    "    sys.modules['src.models.metacontroller'].LRU = LRU\n",
    "    sys.modules['src.models.metacontroller'].BidirectionalHawk = BidirectionalHawk  # ADD THIS\n",
    "    \n",
    "    config = Config(img_size=cfg.img_size, patch_size=cfg.patch_size, embed_dim=cfg.embed_dim, \n",
    "                    n_layers=cfg.n_layers, n_heads=cfg.n_heads, seq_len=cfg.seq_len, frame_stack=cfg.frame_stack)\n",
    "    \n",
    "    base_model = MontezumaVLM(config).to(device).eval()\n",
    "    metacontroller = Metacontroller(config, 256, aux_position_predictor=False).to(device).eval()\n",
    "    \n",
    "    if os.path.exists(cfg.model_checkpoint):\n",
    "        print(f\"Loading base model from {cfg.model_checkpoint}\")\n",
    "        ckpt = torch.load(cfg.model_checkpoint, map_location=device, weights_only=False)\n",
    "        base_model.load_state_dict(ckpt['model_state_dict'])  # NO strict=False\n",
    "        \n",
    "    if os.path.exists(cfg.meta_checkpoint):\n",
    "        print(f\"Loading metacontroller from {cfg.meta_checkpoint}\")\n",
    "        ckpt = torch.load(cfg.meta_checkpoint, map_location=device, weights_only=False)\n",
    "        metacontroller.load_state_dict(ckpt[\"metacontroller_state_dict\"])  # NO strict=False\n",
    "        # === DIAGNOSTIC: Compare checkpoints ===\n",
    "    import hashlib\n",
    "    \n",
    "    def get_weight_fingerprint(model, name):\n",
    "        \"\"\"Get a fingerprint of model weights for comparison\"\"\"\n",
    "        total = 0.0\n",
    "        count = 0\n",
    "        for p in model.parameters():\n",
    "            total += p.data.float().sum().item()\n",
    "            count += p.numel()\n",
    "        return total / count if count > 0 else 0\n",
    "    \n",
    "    vlm_fp = get_weight_fingerprint(base_model, \"VLM\")\n",
    "    meta_fp = get_weight_fingerprint(metacontroller, \"Meta\")\n",
    "    \n",
    "    print(f\"=== CHECKPOINT FINGERPRINTS ===\")\n",
    "    print(f\"VLM weight mean: {vlm_fp:.6f}\")\n",
    "    print(f\"Metacontroller weight mean: {meta_fp:.6f}\")\n",
    "    print(f\"Hypernet[0] weight sum: {metacontroller.hypernet[0].weight.sum().item():.6f}\")\n",
    "    print(f\"Hypernet[-1] weight sum: {metacontroller.hypernet[-1].weight.sum().item():.6f}\")\n",
    "    \n",
    "    # Quick sanity check - run a forward pass\n",
    "    with torch.no_grad():\n",
    "        test_z = torch.randn(1, 8, device=device)\n",
    "        test_params = metacontroller.hypernet(test_z)\n",
    "        print(f\"Hypernet output norm: {test_params.norm().item():.4f}\")\n",
    "    \n",
    "    return base_model, metacontroller, config\n",
    "\n",
    "# ==========================================\n",
    "# 5. TRAINING LOOP\n",
    "# ==========================================\n",
    "def compute_action_positions(num_images: int, n_patches: int, seq_len: int, chunk: int = 8):\n",
    "    \"\"\"\n",
    "    Mirrors MontezumaVLM interleaving:\n",
    "      [patches(img0), actions(0..7), patches(img1), actions(8..15), ...]\n",
    "    Returns:\n",
    "      action_positions: list[int] absolute token indices\n",
    "      T_tokens: total token length of the interleaved sequence\n",
    "    \"\"\"\n",
    "    num_actions = seq_len  # start token + (seq_len-1) previous actions = seq_len tokens\n",
    "    action_positions = []\n",
    "    cur = 0\n",
    "    for i in range(num_images):\n",
    "        cur += n_patches\n",
    "        start = i * chunk\n",
    "        end = min((i + 1) * chunk, num_actions)\n",
    "        if start < num_actions:\n",
    "            k = end - start\n",
    "            action_positions.extend([cur + j for j in range(k)])\n",
    "            cur += k\n",
    "    T_tokens = num_images * n_patches + num_actions\n",
    "    return action_positions, T_tokens\n",
    "\n",
    "\n",
    "def build_tokenwise_adapters(curr_z, metacontroller, config, num_images, seq_len, device):\n",
    "    \"\"\"\n",
    "    Builds A_seq/B_seq of shape (N, T_tokens, D, R) with nonzeros only at action token positions.\n",
    "    This matches the way train_meta.py builds per-token controller sequences.\n",
    "    \"\"\"\n",
    "    N = curr_z.shape[0]\n",
    "    D = config.embed_dim\n",
    "    R = metacontroller.rank\n",
    "    n_patches = (config.img_size // config.patch_size) ** 2\n",
    "\n",
    "    action_pos, T_tokens = compute_action_positions(num_images, n_patches, seq_len)\n",
    "    idx = torch.tensor(action_pos, device=device, dtype=torch.long)  # (T_act,)\n",
    "\n",
    "    # (N, 2, D, R) -> A0/B0: (N, D, R)\n",
    "    params = metacontroller.hypernet(curr_z).view(N, 2, D, R)\n",
    "    A0 = params[:, 0]\n",
    "    B0 = params[:, 1]\n",
    "\n",
    "    # Tokenwise (N, T_tokens, D, R), filled only at action positions\n",
    "    A_seq = torch.zeros(N, T_tokens, D, R, device=device)\n",
    "    B_seq = torch.zeros_like(A_seq)\n",
    "\n",
    "    A_seq[:, idx] = A0.unsqueeze(1).expand(N, idx.numel(), D, R)\n",
    "    B_seq[:, idx] = B0.unsqueeze(1).expand(N, idx.numel(), D, R)\n",
    "\n",
    "    return A_seq, B_seq\n",
    "\n",
    "def fixed_switch_and_beta(fixed_rate, option_prim_steps, device):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      will_switch: (N,) bool\n",
    "      beta:        (N,) float (for logging only)\n",
    "      beta_logit:  (N,1) float (for logging only)\n",
    "    \"\"\"\n",
    "    N = len(option_prim_steps)\n",
    "    fr = float(fixed_rate)\n",
    "\n",
    "    if fr < 1.0:\n",
    "        will_switch = (torch.rand(N, device=device) < fr)\n",
    "        beta_val = fr\n",
    "    else:\n",
    "        k = max(1, int(fr))\n",
    "        will_switch = torch.tensor([(s + 1) >= k for s in option_prim_steps],\n",
    "                                   device=device, dtype=torch.bool)\n",
    "        beta_val = 1.0 / float(k)\n",
    "\n",
    "    beta = torch.full((N,), beta_val, device=device)\n",
    "    beta_logit = torch.logit(beta.clamp(1e-6, 1.0 - 1e-6)).unsqueeze(-1)\n",
    "    return will_switch, beta, beta_logit\n",
    "\n",
    "\n",
    "def train_rl():\n",
    "    control_layer = cfg.n_layers // 2\n",
    "    \n",
    "    def _switch_stats(i):\n",
    "        # safe stats even if no segments yet\n",
    "        segs = t_seg_lens[i]\n",
    "        if len(segs) == 0:\n",
    "            return f\"sw={t_switches[i]} segs=0 prim={t_prim_steps[i]}\"\n",
    "        avg_len = float(np.mean(segs))\n",
    "        med_len = float(np.median(segs))\n",
    "        p90_len = float(np.percentile(segs, 90))\n",
    "        sw_per_step = float(t_switches[i]) / max(1, t_prim_steps[i])\n",
    "        return f\"sw={t_switches[i]} segs={len(segs)} len(avg/med/p90)={avg_len:.1f}/{med_len:.0f}/{p90_len:.0f} sw/step={sw_per_step:.3f} prim={t_prim_steps[i]}\"\n",
    "\n",
    "    \n",
    "    print(f\"Using device: {cfg.device}\")\n",
    "\n",
    "    # 1) Load shared models\n",
    "    shared_models = load_shared_models(cfg.device)\n",
    "    base_model, metacontroller, config = shared_models\n",
    "\n",
    "    # 2) Create internal environments (internal step = execute until beta triggers)\n",
    "    print(f\"Initializing {cfg.num_envs} environments...\")\n",
    "    envs = [\n",
    "        InternalRLWrapper(\n",
    "            device=cfg.device,\n",
    "            beta_threshold=0.75,\n",
    "            seq_len=cfg.seq_len,\n",
    "            render_mode=None,\n",
    "            shared_models=shared_models,\n",
    "        )\n",
    "        for _ in range(cfg.num_envs)\n",
    "    ]\n",
    "\n",
    "    # 3) Initialize policy\n",
    "    agent = Actor(\n",
    "        input_dim=cfg.input_dim,\n",
    "        latent_dim=cfg.latent_dim,\n",
    "        hidden_dim=cfg.hidden_dim,\n",
    "        initial_std=cfg.initial_std,\n",
    "    ).to(cfg.device)\n",
    "    optimizer = optim.Adam(agent.parameters(), lr=cfg.lr, eps=1e-5)\n",
    "\n",
    "    logger = CSVLogger(cfg.log_path)\n",
    "    ep_logger = CSVLogger(cfg.episode_log_path)\n",
    "    global_trajs = 0\n",
    "    global_eps = 0\n",
    "    global_env_eps = 0\n",
    "    start_epoch = 0\n",
    "\n",
    "    if os.path.exists(cfg.save_path):\n",
    "        print(f\"Resuming from {cfg.save_path}...\")\n",
    "        ckpt = torch.load(cfg.save_path, map_location=cfg.device)\n",
    "        agent.load_state_dict(ckpt[\"policy_state_dict\"])\n",
    "        optimizer.load_state_dict(ckpt[\"optimizer_state_dict\"])\n",
    "        start_epoch = ckpt[\"epoch\"]\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Rollout state per env (INTERNAL timescale)\n",
    "    # ------------------------------------------------------------\n",
    "    curr_obs = [None] * cfg.num_envs          # numpy (256,)\n",
    "    curr_z = torch.zeros(cfg.num_envs, cfg.latent_dim, device=cfg.device)\n",
    "    curr_h = [None] * cfg.num_envs            # recurrent state for policy\n",
    "\n",
    "    # per-env stagnation counter (counts INTERNAL steps without reward)\n",
    "    last_reward_timers = [0] * cfg.num_envs\n",
    "\n",
    "    # per-env trajectory buffers (aligned: obs[t] -> act[t] -> rew[t])\n",
    "    t_obs = [[] for _ in range(cfg.num_envs)]       # list of np arrays\n",
    "    t_acts = [[] for _ in range(cfg.num_envs)]      # list of torch tensors (latent_dim,)\n",
    "    t_logps = [[] for _ in range(cfg.num_envs)]     # list of floats\n",
    "    t_rews = [[] for _ in range(cfg.num_envs)]      # list of floats\n",
    "    t_seg_lens = [[] for _ in range(cfg.num_envs)]          # lengths (primitive steps) per option segment\n",
    "    t_switches = [0 for _ in range(cfg.num_envs)]           # count beta-trigger terminations\n",
    "    t_prim_steps = [0 for _ in range(cfg.num_envs)]         # primitive steps elapsed in trajectory\n",
    "    curr_seg_len = [0 for _ in range(cfg.num_envs)]         # current segment primitive length accumulator\n",
    "    t_beta_max = [0.0 for _ in range(cfg.num_envs)]\n",
    "    t_beta_sum = [0.0 for _ in range(cfg.num_envs)]\n",
    "    t_beta_n = [0 for _ in range(cfg.num_envs)]\n",
    "    \n",
    "    t_logit_max = [-1e9 for _ in range(cfg.num_envs)]\n",
    "    t_logit_sum = [0.0 for _ in range(cfg.num_envs)]\n",
    "    t_logit_n = [0 for _ in range(cfg.num_envs)]\n",
    "    # --- add these BEFORE the epoch loop (or at least before collection starts) ---\n",
    "    option_rewards = [0.0 for _ in range(cfg.num_envs)]      # accum primitive rewards inside current option\n",
    "    option_prim_steps = [0   for _ in range(cfg.num_envs)]   # primitive steps inside current option\n",
    "\n",
    "    \n",
    "\n",
    "    # Initial reset + sample initial z (store (s0, z0, logp0); reward comes after executing z0)\n",
    "    print(\"Performing initial reset...\")\n",
    "    for i in range(cfg.num_envs):\n",
    "        obs_np, _ = envs[i].reset()\n",
    "        curr_obs[i] = obs_np\n",
    "\n",
    "        obs_t = torch.tensor(obs_np, dtype=torch.float32, device=cfg.device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            dist, h_next = agent.step(obs_t, None)\n",
    "            z = dist.sample()\n",
    "            logp = dist.log_prob(z).sum(dim=-1)  # shape (1,)\n",
    "\n",
    "        curr_z[i] = z.squeeze(0)\n",
    "        curr_h[i] = h_next\n",
    "\n",
    "        t_obs[i].append(obs_np)                 # s0\n",
    "        t_acts[i].append(z.squeeze(0).cpu())    # z0\n",
    "        t_logps[i].append(float(logp.item()))   # log Ï€(z0|s0)\n",
    "\n",
    "    print(f\"Starting internal RL (Max no-reward internal steps: {cfg.max_steps_without_reward})...\")\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Training loop\n",
    "    # ------------------------------------------------------------\n",
    "    for epoch in range(start_epoch, cfg.epochs):\n",
    "        epoch_start = time.time()\n",
    "\n",
    "        # batch buffers (variable-length trajectories)\n",
    "        b_obs, b_acts, b_oldlogps, b_advs = [], [], [], []\n",
    "        batch_total_rewards = []\n",
    "        trajs_collected = 0\n",
    "\n",
    "        # ----------------------------\n",
    "        # COLLECT trajectories\n",
    "        # ----------------------------\n",
    "        with torch.no_grad():\n",
    "            while trajs_collected < cfg.batch_size:\n",
    "\n",
    "                # ---- (B) gather model inputs (PRE-STEP) ----\n",
    "                frames_list, actions_list = [], []\n",
    "                for env in envs:\n",
    "                    f, a = env._get_model_inputs()\n",
    "                    frames_list.append(f)\n",
    "                    actions_list.append(a)\n",
    "                \n",
    "                all_frames  = torch.cat(frames_list, dim=0)   # (N, NumImg, 4, 84, 84)\n",
    "                all_actions = torch.cat(actions_list, dim=0)  # (N, seq_len)\n",
    "\n",
    "                if cfg.fixed_rate != 0.0:\n",
    "                    will_switch, beta, beta_logit = fixed_switch_and_beta(\n",
    "                        fixed_rate=cfg.fixed_rate,\n",
    "                        option_prim_steps=option_prim_steps,\n",
    "                        device=cfg.device,\n",
    "                    )\n",
    "                else:\n",
    "                    # ============================================================\n",
    "                    # PASS 1 (CLEAN): compute e_t for beta + h_switch update\n",
    "                    # ============================================================\n",
    "                    out_clean = base_model(\n",
    "                        all_frames,\n",
    "                        all_actions,\n",
    "                        return_residuals=True,\n",
    "                        adapter_params=None,                      # IMPORTANT: CLEAN\n",
    "                    )\n",
    "                    \n",
    "                    lap = out_clean[\"last_action_pos\"]            # scalar int\n",
    "                    e_t_clean = out_clean[\"residuals\"][control_layer][:, lap, :]  # (N, D)\n",
    "                    \n",
    "                    h_prev = torch.cat([env.h_switch for env in envs], dim=0)     # (N, n_h)\n",
    "                    switch_inp = torch.cat([e_t_clean, h_prev, curr_z], dim=1)    # (N, D+n_h+Z)\n",
    "                    \n",
    "                    beta_logit = metacontroller.switch_net(switch_inp)            # (N, 1)\n",
    "                    beta = torch.sigmoid(beta_logit).squeeze(-1)                  # (N,)\n",
    "                    \n",
    "                    h_next = metacontroller.history_gru(e_t_clean, h_prev)        # (N, n_h)\n",
    "                    for i in range(cfg.num_envs):\n",
    "                        envs[i].h_switch = h_next[i:i+1]\n",
    "                    \n",
    "                    will_switch = beta > envs[0].beta_threshold                   # (N,)\n",
    "\n",
    "                # ============================================================\n",
    "                # PASS 2 (CONTROLLED): sample primitive action logits under adapters\n",
    "                # ============================================================\n",
    "                A_seq, B_seq = build_tokenwise_adapters(\n",
    "                    curr_z=curr_z,\n",
    "                    metacontroller=metacontroller,\n",
    "                    config=config,\n",
    "                    num_images=envs[0].num_images,\n",
    "                    seq_len=cfg.seq_len,\n",
    "                    device=cfg.device,\n",
    "                )\n",
    "                \n",
    "                out_ctrl = base_model(\n",
    "                    all_frames,\n",
    "                    all_actions,\n",
    "                    return_residuals=False,\n",
    "                    adapter_params=(A_seq, B_seq, control_layer, \"action_tokens\"),\n",
    "                )\n",
    "\n",
    "                \n",
    "                probs = torch.softmax(out_ctrl[\"logits\"], dim=-1)             # (N, n_actions)\n",
    "                prim_actions = torch.multinomial(probs, 1).squeeze(-1)        # (N,)\n",
    "                \n",
    "                # ---- (D) step all envs one primitive step (using sampled actions) ----\n",
    "                done_mask = torch.zeros(cfg.num_envs, device=cfg.device, dtype=torch.bool)\n",
    "                for i in range(cfg.num_envs):\n",
    "                    a = int(prim_actions[i].item())\n",
    "                    r, terminated, truncated, _ = envs[i].primitive_step(a)\n",
    "                    done = terminated or truncated\n",
    "                    done_mask[i] = done\n",
    "                \n",
    "                    # primitive-step accounting\n",
    "                    option_rewards[i] += float(r)\n",
    "                    option_prim_steps[i] += 1\n",
    "                    t_prim_steps[i] += 1\n",
    "                    curr_seg_len[i] += 1\n",
    "                \n",
    "                # ---- (G) handle option termination / trajectory termination ----\n",
    "                for i in range(cfg.num_envs):\n",
    "                    # beta/logit stats (per trajectory)\n",
    "                    b = float(beta[i].item())\n",
    "                    l = float(beta_logit[i].item())\n",
    "                    t_beta_max[i] = max(t_beta_max[i], b)\n",
    "                    t_beta_sum[i] += b\n",
    "                    t_beta_n[i] += 1\n",
    "                    t_logit_max[i] = max(t_logit_max[i], l)\n",
    "                    t_logit_sum[i] += l\n",
    "                    t_logit_n[i] += 1\n",
    "                \n",
    "                    done = bool(done_mask[i].item())\n",
    "                    ended_by_beta = bool(will_switch[i].item()) and (not done)\n",
    "                \n",
    "                    if ended_by_beta or done:\n",
    "                        if ended_by_beta:\n",
    "                            t_switches[i] += 1\n",
    "                        t_seg_lens[i].append(curr_seg_len[i])\n",
    "                        curr_seg_len[i] = 0\n",
    "                \n",
    "                        seg_reward = option_rewards[i]\n",
    "                        option_rewards[i] = 0.0\n",
    "                        option_prim_steps[i] = 0\n",
    "                \n",
    "                        t_rews[i].append(float(seg_reward))\n",
    "                \n",
    "                        if seg_reward > 0:\n",
    "                            last_reward_timers[i] = 0\n",
    "                        else:\n",
    "                            last_reward_timers[i] += 1\n",
    "                        timeout = last_reward_timers[i] >= cfg.max_steps_without_reward\n",
    "                \n",
    "                        traj_complete = done or timeout\n",
    "                        if traj_complete:\n",
    "                            T = len(t_rews[i])\n",
    "                            assert len(t_obs[i]) == T and len(t_acts[i]) == T and len(t_logps[i]) == T\n",
    "                \n",
    "                            if trajs_collected < cfg.batch_size:\n",
    "                                total_r = float(sum(t_rews[i]))\n",
    "                                batch_total_rewards.append(total_r)\n",
    "                \n",
    "                                b_obs.append(torch.tensor(np.array(t_obs[i]), dtype=torch.float32, device=cfg.device))\n",
    "                                b_acts.append(torch.stack(t_acts[i]).to(cfg.device))\n",
    "                                b_oldlogps.append(torch.tensor(t_logps[i], dtype=torch.float32, device=cfg.device))\n",
    "                                b_advs.append(torch.full((T,), total_r, dtype=torch.float32, device=cfg.device))\n",
    "                \n",
    "                                trajs_collected += 1\n",
    "                \n",
    "                                beta_mean = t_beta_sum[i] / max(1, t_beta_n[i])\n",
    "                                logit_mean = t_logit_sum[i] / max(1, t_logit_n[i])\n",
    "                                status = \"DONE\" if done else \"TIMEOUT\"\n",
    "                                print(\n",
    "                                    f\"\\n Traj {trajs_collected}/{cfg.batch_size} | R: {total_r:.1f} | L: {T} | {status} | \"\n",
    "                                    f\"sw={t_switches[i]} segs={len(t_seg_lens[i])} \"\n",
    "                                    f\"len(avg/med/p90)={np.mean(t_seg_lens[i]):.1f}/{np.median(t_seg_lens[i]):.0f}/{np.percentile(t_seg_lens[i],90):.0f} \"\n",
    "                                    f\"sw/step={t_switches[i]/max(1,t_prim_steps[i]):.3f} prim={t_prim_steps[i]} | \"\n",
    "                                    f\"beta(max/mean)={t_beta_max[i]:.3f}/{beta_mean:.3f} logit(max/mean)={t_logit_max[i]:.3f}/{logit_mean:.3f}\",\n",
    "                                    end=\"\"\n",
    "                                )\n",
    "\n",
    "                                global_eps += 1\n",
    "                                if done:\n",
    "                                    global_env_eps += 1\n",
    "                                \n",
    "                                used_in_batch = int(trajs_collected < cfg.batch_size)\n",
    "                                \n",
    "                                ep_logger.log(\n",
    "                                    {\n",
    "                                        \"episode\": global_eps,                 # DONE+TIMEOUT\n",
    "                                        \"env_episode\": global_env_eps,         # DONE only\n",
    "                                        \"epoch\": epoch + 1,\n",
    "                                        \"used_in_batch\": used_in_batch,\n",
    "                                        \"env_id\": i,\n",
    "                                        \"status\": status,                      # \"DONE\" or \"TIMEOUT\"\n",
    "                                        \"return\": total_r,\n",
    "                                        \"L_options\": T,                        # number of option decisions\n",
    "                                        \"prim_steps\": t_prim_steps[i],\n",
    "                                        \"switches\": t_switches[i],\n",
    "                                        \"segs\": len(t_seg_lens[i]),\n",
    "                                        \"seg_len_avg\": float(np.mean(t_seg_lens[i])) if len(t_seg_lens[i]) else 0.0,\n",
    "                                        \"seg_len_med\": float(np.median(t_seg_lens[i])) if len(t_seg_lens[i]) else 0.0,\n",
    "                                        \"seg_len_p90\": float(np.percentile(t_seg_lens[i], 90)) if len(t_seg_lens[i]) else 0.0,\n",
    "                                        \"beta_max\": float(t_beta_max[i]),\n",
    "                                        \"beta_mean\": float(beta_mean),\n",
    "                                        \"logit_max\": float(t_logit_max[i]),\n",
    "                                        \"logit_mean\": float(logit_mean),\n",
    "                                    },\n",
    "                                    step=global_eps,\n",
    "                                    flush_every=50,   # reduce IO\n",
    "                                )\n",
    "                \n",
    "                            # reset env + per-traj buffers\n",
    "                            obs_np, _ = envs[i].reset()\n",
    "                            curr_h[i] = None\n",
    "                            last_reward_timers[i] = 0\n",
    "                \n",
    "                            t_obs[i], t_acts[i], t_logps[i], t_rews[i] = [], [], [], []\n",
    "                \n",
    "                            t_seg_lens[i] = []\n",
    "                            t_switches[i] = 0\n",
    "                            t_prim_steps[i] = 0\n",
    "                            curr_seg_len[i] = 0\n",
    "                \n",
    "                            t_beta_max[i] = 0.0\n",
    "                            t_beta_sum[i] = 0.0\n",
    "                            t_beta_n[i] = 0\n",
    "                            t_logit_max[i] = -1e9\n",
    "                            t_logit_sum[i] = 0.0\n",
    "                            t_logit_n[i] = 0\n",
    "                \n",
    "                            # sample initial z for new trajectory and store (s0,z0,logp0)\n",
    "                            obs_t = torch.tensor(obs_np, dtype=torch.float32, device=cfg.device).unsqueeze(0)\n",
    "                            dist, h_next_pol = agent.step(obs_t, None)\n",
    "                            z = dist.sample()\n",
    "                            logp = dist.log_prob(z).sum(dim=-1)\n",
    "                \n",
    "                            curr_z[i] = z.squeeze(0)\n",
    "                            curr_h[i] = h_next_pol\n",
    "                \n",
    "                            t_obs[i].append(obs_np)\n",
    "                            t_acts[i].append(z.squeeze(0).cpu())\n",
    "                            t_logps[i].append(float(logp.item()))\n",
    "                \n",
    "                        else:\n",
    "                            # option boundary: sample next z using CURRENT embedding (post-step state)\n",
    "                            # NOTE: this does a VLM forward, but only at boundaries (much rarer than every primitive step).\n",
    "                            obs_np = envs[i]._get_current_embedding()\n",
    "                            obs_t = torch.tensor(obs_np, dtype=torch.float32, device=cfg.device).unsqueeze(0)\n",
    "                \n",
    "                            dist, h_next_pol = agent.step(obs_t, curr_h[i])\n",
    "                            z = dist.sample()\n",
    "                            logp = dist.log_prob(z).sum(dim=-1)\n",
    "                \n",
    "                            curr_z[i] = z.squeeze(0)\n",
    "                            curr_h[i] = h_next_pol\n",
    "                \n",
    "                            t_obs[i].append(obs_np)\n",
    "                            t_acts[i].append(z.squeeze(0).cpu())\n",
    "                            t_logps[i].append(float(logp.item()))\n",
    "                    if trajs_collected >= cfg.batch_size:\n",
    "                        break\n",
    "\n",
    "        print(\"\")  # newline after collection\n",
    "\n",
    "        # quick z stats (not used for learning, just logging)\n",
    "        z_diversity = curr_z.std(dim=0).mean().item()\n",
    "        z_norm = curr_z.norm(dim=1).mean().item()\n",
    "        print(f\"Z diversity: {z_diversity:.4f} | Z norm: {z_norm:.4f}\")\n",
    "\n",
    "        # ----------------------------\n",
    "        # UPDATE (PPO, recurrent unroll)\n",
    "        # ----------------------------\n",
    "        if not batch_total_rewards:\n",
    "            continue\n",
    "\n",
    "        collection_time = time.time() - epoch_start\n",
    "        all_r = torch.tensor(batch_total_rewards, device=cfg.device)\n",
    "        mean_r = all_r.mean()\n",
    "        max_r = all_r.max().item()\n",
    "        std_r = all_r.std() + 1e-8\n",
    "\n",
    "        # normalize advantages trajectory-wise (same value repeated per step)\n",
    "        for j in range(len(b_advs)):\n",
    "            b_advs[j] = (b_advs[j] - mean_r) / std_r\n",
    "\n",
    "        # PPO epochs over same batch\n",
    "        ppo_epochs = 4\n",
    "        clip_eps = 0.2\n",
    "\n",
    "        total_loss_val = 0.0\n",
    "\n",
    "        for _ in range(ppo_epochs):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            total_pg = 0.0\n",
    "            total_ent = 0.0\n",
    "            total_steps = 0\n",
    "\n",
    "            # Unroll each trajectory with agent.step() to compute new logprobs correctly for a recurrent policy\n",
    "            for traj_obs, traj_act, traj_oldlogp, traj_adv in zip(b_obs, b_acts, b_oldlogps, b_advs):\n",
    "                h = None\n",
    "                T = traj_obs.shape[0]\n",
    "\n",
    "                for t in range(T):\n",
    "                    obs_t = traj_obs[t].unsqueeze(0)          # (1,256)\n",
    "                    act_t = traj_act[t].unsqueeze(0)          # (1,8)\n",
    "\n",
    "                    dist, h = agent.step(obs_t, h)\n",
    "                    new_logp = dist.log_prob(act_t).sum(dim=-1)       # (1,)\n",
    "                    old_logp = traj_oldlogp[t].view(1)                # (1,)\n",
    "                    adv_t = traj_adv[t].view(1)                       # (1,)\n",
    "                    ent = dist.entropy().sum(dim=-1)                  # (1,)\n",
    "\n",
    "                    logratio = new_logp - old_logp\n",
    "                    ratio = torch.exp(logratio)\n",
    "\n",
    "                    pg1 = -adv_t * ratio\n",
    "                    pg2 = -adv_t * torch.clamp(ratio, 1.0 - clip_eps, 1.0 + clip_eps)\n",
    "                    pg = torch.maximum(pg1, pg2)\n",
    "\n",
    "                    total_pg = total_pg + pg\n",
    "                    total_ent = total_ent + ent\n",
    "                    total_steps += 1\n",
    "\n",
    "            # mean over all timesteps in batch\n",
    "            pg_loss = total_pg / max(total_steps, 1)\n",
    "            ent_mean = total_ent / max(total_steps, 1)\n",
    "            total_loss = pg_loss - cfg.entropy_coef * ent_mean\n",
    "\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(agent.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss_val = float(total_loss.item())\n",
    "\n",
    "        global_trajs += trajs_collected\n",
    "\n",
    "        logger.log(\n",
    "            {\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"step\": global_trajs,\n",
    "                \"reward\": float(mean_r.item()),\n",
    "                \"loss\": total_loss_val,\n",
    "                \"z_diversity\": z_diversity,\n",
    "                \"z_norm\": z_norm,\n",
    "            },\n",
    "            step=epoch + 1,\n",
    "            flush_every=5,\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1} | Mean: {mean_r.item():.2f} | Max: {max_r:.0f} | \"\n",
    "            f\"Loss: {total_loss_val:.4f} | Z-div: {z_diversity:.4f} | Coll: {collection_time:.1f}s\"\n",
    "        )\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"policy_state_dict\": agent.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"epoch\": epoch + 1,\n",
    "                },\n",
    "                cfg.save_path,\n",
    "            )\n",
    "            print(f\"Saved checkpoint to {cfg.save_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_rl()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9351863,
     "sourceId": 14819336,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
